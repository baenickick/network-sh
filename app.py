import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from utils.text_preprocessing import TextPreprocessor
from utils.network_analysis import NetworkAnalyzer
from utils.topic_modeling import TopicModeler

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìˆ™ë°• í›„ê¸° í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¨",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'data' not in st.session_state:
    st.session_state.data = None
if 'preprocessor' not in st.session_state:
    st.session_state.preprocessor = TextPreprocessor()
if 'network_analyzer' not in st.session_state:
    st.session_state.network_analyzer = NetworkAnalyzer()
if 'topic_modeler' not in st.session_state:
    st.session_state.topic_modeler = TopicModeler()

# ë©”ì¸ ì œëª©
st.title("ğŸ¨ ìˆ™ë°• í›„ê¸° í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.markdown("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê³¼ í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì¸í„°ë™í‹°ë¸Œ í…ìŠ¤íŠ¸ ë¶„ì„")
st.markdown("---")

# ì‚¬ì´ë“œë°” - íŒŒì¼ ì—…ë¡œë“œ ë° ì„¤ì •
with st.sidebar:
    st.header("ğŸ“ 1. CSV íŒŒì¼ ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ìˆëŠ” CSV íŒŒì¼ ì—…ë¡œë“œ",
        type=['csv'],
        help="Context ì»¬ëŸ¼ì— ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
    )
    
    if uploaded_file is not None:
        try:
            st.session_state.data = pd.read_csv(uploaded_file)
            st.success(f"âœ… {len(st.session_state.data)} í–‰ì˜ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # Context ì»¬ëŸ¼ í™•ì¸
            if 'Context' not in st.session_state.data.columns:
                st.error("âŒ 'Context' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                available_columns = list(st.session_state.data.columns)
                st.write("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼:", available_columns)
                st.stop()
                
            # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            with st.expander("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                st.dataframe(st.session_state.data.head())
                
        except Exception as e:
            st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            st.stop()
    
    st.markdown("---")
    
    # í‚¤ì›Œë“œ í•„í„°ë§ ì˜µì…˜
    st.header("ğŸ” 2. í‚¤ì›Œë“œ í•„í„°ë§ ì„¤ì •")
    
    exclude_keywords = st.text_area(
        "ì œì™¸í•  í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        placeholder="í˜¸í…”, ë°©, ì˜ˆì‹œí‚¤ì›Œë“œ",
        help="ë„¤íŠ¸ì›Œí¬ ë¶„ì„ê³¼ í† í”½ ëª¨ë¸ë§ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
    )
    
    include_keywords = st.text_area(
        "ê¼­ í¬í•¨í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        placeholder="ì„œë¹„ìŠ¤, ì²­ê²°, ìœ„ì¹˜",
        help="ì´ í‚¤ì›Œë“œë“¤ì„ í¬í•¨í•œ ë¦¬ë·°ë§Œ ë¶„ì„í•©ë‹ˆë‹¤. ë¹„ì›Œë‘ë©´ ì „ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
    )
    
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    exclude_words = [word.strip() for word in exclude_keywords.split(',') if word.strip()] if exclude_keywords else None
    include_words = [word.strip() for word in include_keywords.split(',') if word.strip()] if include_keywords else None
    
    # í•„í„°ë§ ìƒíƒœ í‘œì‹œ
    if exclude_words:
        st.write("ğŸš« **ì œì™¸ í‚¤ì›Œë“œ:**", ", ".join(exclude_words))
    if include_words:
        st.write("âœ… **í¬í•¨ í‚¤ì›Œë“œ:**", ", ".join(include_words))
    
    st.markdown("---")
    
    # ë¶„ì„ íŒŒë¼ë¯¸í„°
    st.header("âš™ï¸ 3. ë¶„ì„ ì„¤ì •")
    n_topics = st.slider("í† í”½ ëª¨ë¸ë§ ì£¼ì œ ìˆ˜", 3, 10, 5)
    min_frequency = st.slider("ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ì—°ê²° ë¹ˆë„", 1, 5, 2)

# ë©”ì¸ ì½˜í…ì¸ 
if st.session_state.data is not None:
    
    # ì‹¤ì‹œê°„ ë¶„ì„ í•¨ìˆ˜
    def perform_realtime_analysis():
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        texts = [st.session_state.preprocessor.clean_text(text) for text in st.session_state.data['Context']]
        texts = [text for text in texts if text.strip()]  # ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°
        
        if not texts:
            return None, None, None, None
        
        # ë™ì‹œì¶œí˜„ í–‰ë ¬ ìƒì„±
        cooccurrence = st.session_state.preprocessor.get_cooccurrence_matrix(
            texts, exclude_words, include_words
        )
        
        # ë„¤íŠ¸ì›Œí¬ ë¶„ì„
        network_graph = st.session_state.network_analyzer.create_network(
            cooccurrence, min_frequency
        )
        
        # í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¤€ë¹„
        filtered_texts = []
        for text in texts:
            keywords = st.session_state.preprocessor.extract_keywords(
                text, exclude_words, include_words
            )
            if keywords:  # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                filtered_texts.append(' '.join(keywords))
        
        # í† í”½ ëª¨ë¸ë§
        topic_model = None
        if len(filtered_texts) >= 5:  # ìµœì†Œ 5ê°œ ë¬¸ì„œ í•„ìš”
            topic_model = st.session_state.topic_modeler.fit_lda_model(
                filtered_texts, n_topics
            )
        
        return network_graph, filtered_texts, len(texts), topic_model
    
    # ë¶„ì„ ì‹¤í–‰
    with st.spinner("ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘..."):
        network_graph, filtered_texts, total_texts, topic_model = perform_realtime_analysis()
    
    if network_graph is None:
        st.warning("âš ï¸ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ í•„í„°ë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
        st.stop()
    
    # ë¶„ì„ ê²°ê³¼ ìš”ì•½
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“„ ì „ì²´ ë¦¬ë·°", len(st.session_state.data))
    with col2:
        st.metric("ğŸ“ ë¶„ì„ëœ ë¦¬ë·°", total_texts)
    with col3:
        st.metric("ğŸ”— ë„¤íŠ¸ì›Œí¬ í‚¤ì›Œë“œ", len(network_graph.nodes()) if network_graph else 0)
    with col4:
        st.metric("ğŸ“Š í† í”½ ìˆ˜", n_topics if topic_model else 0)
    
    # ê²°ê³¼ í‘œì‹œ íƒ­
    tab1, tab2, tab3 = st.tabs(["ğŸ•¸ï¸ ë„¤íŠ¸ì›Œí¬ ë¶„ì„", "ğŸ“Š í† í”½ ëª¨ë¸ë§", "ğŸ“ˆ ë¶„ì„ ê²°ê³¼í‘œ"])
    
    with tab1:
        st.subheader("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        st.markdown("í‚¤ì›Œë“œ ê°„ì˜ ë™ì‹œì¶œí˜„ ê´€ê³„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. **í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤!**")
        
        # ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”
        network_fig = st.session_state.network_analyzer.create_interactive_network_plot()
        st.plotly_chart(network_fig, use_container_width=True)
        
        # ì¤‘ì‹¬ì„± ì§€í‘œ í…Œì´ë¸”
        centrality_df = st.session_state.network_analyzer.calculate_centrality_metrics()
        if not centrality_df.empty:
            st.subheader("ğŸ“‹ í‚¤ì›Œë“œ ì¤‘ì‹¬ì„± ì§€í‘œ")
            st.markdown("ê° í‚¤ì›Œë“œì˜ ë„¤íŠ¸ì›Œí¬ ë‚´ ì˜í–¥ë ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
            st.dataframe(
                centrality_df.head(10), 
                use_container_width=True,
                column_config={
                    "keyword": "í‚¤ì›Œë“œ",
                    "degree_centrality": "ì—°ê²° ì¤‘ì‹¬ì„±",
                    "betweenness_centrality": "ë§¤ê°œ ì¤‘ì‹¬ì„±", 
                    "closeness_centrality": "ê·¼ì ‘ ì¤‘ì‹¬ì„±",
                    "eigenvector_centrality": "ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„±"
                }
            )
    
    with tab2:
        st.subheader("í† í”½ ëª¨ë¸ë§ ë¶„ì„")
        st.markdown("LDA ê¸°ë²•ì„ í†µí•´ ìˆ¨ê²¨ì§„ ì£¼ì œë¥¼ ë°œê²¬í•©ë‹ˆë‹¤. **í‚¤ì›Œë“œ í•„í„°ê°€ ì‹¤ì‹œê°„ ë°˜ì˜ë©ë‹ˆë‹¤!**")
        
        if topic_model and len(filtered_texts) >= 5:
            # í† í”½ ë¶„í¬ ì°¨íŠ¸
            topic_dist_fig = st.session_state.topic_modeler.create_topic_distribution_plot()
            st.plotly_chart(topic_dist_fig, use_container_width=True)
            
            # í† í”½ë³„ í‚¤ì›Œë“œ í…Œì´ë¸”
            topics_df = st.session_state.topic_modeler.get_topics_dataframe(n_words=8)
            if not topics_df.empty:
                st.subheader("ğŸ“‹ í† í”½ë³„ ì£¼ìš” í‚¤ì›Œë“œ")
                
                # í† í”½ ìš”ì•½
                topic_summary = st.session_state.topic_modeler.get_topic_keywords_summary()
                for topic_name, keywords in topic_summary.items():
                    st.markdown(f"**{topic_name}**: {keywords}")
                
                st.markdown("---")
                
                # ìƒì„¸ í† í”½ í…Œì´ë¸”
                for topic in topics_df['topic'].unique():
                    topic_data = topics_df[topics_df['topic'] == topic].head(8)
                    
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        st.markdown(f"### {topic}")
                    with col2:
                        st.dataframe(
                            topic_data[['keyword', 'weight']].round(4),
                            hide_index=True,
                            column_config={
                                "keyword": "í‚¤ì›Œë“œ",
                                "weight": "ê°€ì¤‘ì¹˜"
                            }
                        )
        else:
            st.warning("ğŸ” í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ 5ê°œ ë¬¸ì„œ í•„ìš”)")
            st.info("í‚¤ì›Œë“œ í•„í„°ë¥¼ ì¡°ì •í•˜ê±°ë‚˜ ë” ë§ì€ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ë³´ì„¸ìš”.")
    
    with tab3:
        st.subheader("ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
        
        # í•„í„°ë§ ì •ë³´ í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### ğŸ“ˆ ë¶„ì„ í†µê³„")
            analysis_stats = pd.DataFrame({
                'í•­ëª©': ['ì—…ë¡œë“œëœ ë¦¬ë·° ìˆ˜', 'ë¶„ì„ëœ ë¦¬ë·° ìˆ˜', 'ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜', 'ë„¤íŠ¸ì›Œí¬ ë…¸ë“œ ìˆ˜', 'ì„¤ì •ëœ í† í”½ ìˆ˜'],
                'ê°’': [
                    len(st.session_state.data),
                    total_texts,
                    len(set([word for text in filtered_texts for word in text.split()])) if filtered_texts else 0,
                    len(network_graph.nodes()) if network_graph else 0,
                    n_topics if topic_model else 0
                ]
            })
            st.dataframe(analysis_stats, hide_index=True)
        
        with col2:
            st.markdown("### ğŸ” í•„í„° ì„¤ì •")
            filter_info = []
            
            if include_words:
                filter_info.append(f"**í¬í•¨ í‚¤ì›Œë“œ**: {', '.join(include_words)}")
            else:
                filter_info.append("**í¬í•¨ í‚¤ì›Œë“œ**: ì „ì²´ ë°ì´í„° ë¶„ì„")
                
            if exclude_words:
                filter_info.append(f"**ì œì™¸ í‚¤ì›Œë“œ**: {', '.join(exclude_words)}")
            else:
                filter_info.append("**ì œì™¸ í‚¤ì›Œë“œ**: ì—†ìŒ")
            
            for info in filter_info:
                st.markdown(info)
        
        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
        st.markdown("### ğŸ’¾ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            if not centrality_df.empty:
                csv_network = centrality_df.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“Š ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                    data=csv_network,
                    file_name=f"network_analysis_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv",
                    mime="text/csv"
                )
        
        with col2:
            # í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
            if topic_model:
                topics_df = st.session_state.topic_modeler.get_topics_dataframe()
                if not topics_df.empty:
                    csv_topics = topics_df.to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ğŸ“ˆ í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                        data=csv_topics,
                        file_name=f"topic_modeling_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.csv",
                        mime="text/csv"
                    )

else:
    # ì´ˆê¸° í™”ë©´
    st.info("ğŸ‘ˆ **ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”!**")
    
    # ì‚¬ìš© ê°€ì´ë“œ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### ğŸ“š ì‚¬ìš© ë°©ë²•
        1. **CSV íŒŒì¼ ì—…ë¡œë“œ**: Context ì»¬ëŸ¼ì— ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ìˆëŠ” íŒŒì¼
        2. **ì œì™¸ í‚¤ì›Œë“œ ì…ë ¥**: ë¶„ì„ì—ì„œ ë¹¼ê³  ì‹¶ì€ í‚¤ì›Œë“œë“¤
        3. **í¬í•¨ í‚¤ì›Œë“œ ì…ë ¥**: ì´ í‚¤ì›Œë“œê°€ ìˆëŠ” ë¦¬ë·°ë§Œ ë¶„ì„ (ì„ íƒì‚¬í•­)
        4. **ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼**: í‚¤ì›Œë“œ ì…ë ¥ ì‹œ ì¦‰ì‹œ ì°¨íŠ¸ ì—…ë°ì´íŠ¸
        5. **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**: ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        """)
    
    with col2:
        st.markdown("""
        ### âœ¨ ì£¼ìš” ê¸°ëŠ¥
        - ğŸ”„ **ì‹¤ì‹œê°„ ì¸í„°ë™í‹°ë¸Œ ë¶„ì„**
        - ğŸ•¸ï¸ **í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”**  
        - ğŸ“Š **í† í”½ ëª¨ë¸ë§ (LDA)**
        - ğŸ“ˆ **ì¤‘ì‹¬ì„± ì§€í‘œ ë¶„ì„**
        - ğŸ’¾ **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**
        - ğŸ¯ **í‚¤ì›Œë“œ í•„í„°ë§**
        """)
    
    # ìƒ˜í”Œ ë°ì´í„° ì•ˆë‚´
    st.markdown("---")
    st.markdown("### ğŸ“ ìƒ˜í”Œ CSV íŒŒì¼ í˜•ì‹")
    sample_df = pd.DataFrame({
        'Context': [
            'í˜¸í…”ì´ ê¹¨ë—í•˜ê³  ì§ì›ë“¤ì´ ë§¤ìš° ì¹œì ˆí–ˆìŠµë‹ˆë‹¤. ì¡°ì‹ë„ ë§›ìˆì—ˆì–´ìš”.',
            'ìœ„ì¹˜ê°€ ì¢‹ê³  ê°€ê²©ì´ í•©ë¦¬ì ì´ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì— ë˜ ì˜¬ ê²ƒ ê°™ì•„ìš”.',
            'ë°©ì´ ì¢ì•˜ì§€ë§Œ ì‹œì„¤ì€ ê¹”ë”í–ˆìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ê°€ ì¢‹ì•˜ì–´ìš”.',
            'ë·°ê°€ ì •ë§ ì¢‹ì•˜ê³  ì¹¨ëŒ€ê°€ í¸ì•ˆí–ˆìŠµë‹ˆë‹¤. ì¶”ì²œí•©ë‹ˆë‹¤!'
        ]
    })
    st.dataframe(sample_df, use_container_width=True)
    
    # ìƒ˜í”Œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    sample_csv = sample_df.to_csv(index=False, encoding='utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ìƒ˜í”Œ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
        data=sample_csv,
        file_name="sample_hotel_reviews.csv",
        mime="text/csv"
    )

# í‘¸í„°
st.markdown("---")
st.markdown("**ğŸ’¡ Tip**: í‚¤ì›Œë“œë¥¼ ì…ë ¥/ìˆ˜ì •í•  ë•Œë§ˆë‹¤ ë¶„ì„ ê²°ê³¼ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤!")
st.markdown("Built with â¤ï¸ using Streamlit | ë„ì‹œ ë¸Œëœë”© ì—°êµ¬ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬")

